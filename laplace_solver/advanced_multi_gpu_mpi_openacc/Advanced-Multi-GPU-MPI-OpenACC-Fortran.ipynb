{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Multi GPU Programming with MPI and OpenACC\n",
    "\n",
    "In this self-paced, hands-on lab, you will learn how to improve a multi GPU MPI+OpenACC program. It is a followup lab of the **Introduction to Multi GPU Programming with MPI and OpenACC** lab. Knowledge on how to program multiple GPUs with MPI and OpenACC is a prerequisite.  The topics covered by this lab are:\n",
    "\n",
    "* Overlapping communication with computation to hide communication times\n",
    "* Handling noncontiguous halo updates with a 2D tiled domain decomposition\n",
    "\n",
    "This Lab was created by Jiri Kraus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following timer counts down to a five minute warning before the lab instance shuts down.  You should get a pop up at the five minute warning reminding you to save your work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"timer\" src=\"timer/timer.html\" width=\"100%\" height=\"120px\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Before we begin, let's verify [WebSockets](http://en.wikipedia.org/wiki/WebSocket) are working on your system.  To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above.  If all goes well, you should see some output returned below the grey cell.  If not, please consult the [Self-paced Lab Troubleshooting FAQ](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) to debug the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"The answer should be three: \" + str(1+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the GPUs running on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The following video will explain the infrastructure we are using for this self-paced lab, as well as give some tips on it's usage.  If you've never taken a lab on this system before, it's highly recommended that you watch this short video first.<br><br>\n",
    "<div align=\"center\"><iframe width=\"640\" height=\"390\" src=\"http://www.youtube.com/embed/ZMrDaLSFqpY\" frameborder=\"0\" allowfullscreen></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "You have propably scaled you program to multiple GPUs to get a faster time to solution or to be able to solve larger problems, but the multi GPU scalability is below expectations or you want to futher improve the scalability. There are two major reasons why the scalability of a multi GPU program does not meet expectations\n",
    "\n",
    "* Load Imbalances, i.e. some GPUs have to compute signficantly more work than others. This often results in idle GPUs waiting for the busy GPUs to finish.\n",
    "* Parallel Overhead caused by the necessary coordination of work of multiple GPUs, e.g. halo exchanges.\n",
    "\n",
    "This lab will cover how to lower the impact of the parallel overhead.\n",
    "\n",
    "## Scalability Metrics For Success\n",
    "\n",
    "Like in the **Introduction to Multi GPU Programming with MPI and OpenACC** lab the success of improvments for the multi GPU parallelization is measured with the following metrics. The provided tasks automatically print these metrics out at the end of each lab section/execution.\n",
    "\n",
    "* Serial Time: $\\Large{T_{s}}$  - how long it takes to run the problem with a single GPU\n",
    "* Parallel Time: $\\Large{T_{p}}$  - how long it takes to run the problem on multiple GPUs\n",
    "* Number of Processors:  $P$  - the number of processors (GPUs) operating in parallel\n",
    "* Speedup: $\\Large{S = \\frac{T_{s}}{T_{p}}}$ - How much faster the parallel version is versus the serial version.\n",
    " * The ideal speed up is $P$.\n",
    "* Efficiency: $\\Large{E = \\frac{S}{P}}$ - How efficiently the processors are being used.\n",
    " * The ideal efficiency is $1$.\n",
    "\n",
    "##Basics\n",
    "\n",
    "### CUDA-aware MPI\n",
    "\n",
    "A CUDA-aware MPI implementation allows you to exchange data directly to and from the GPUs buffers involved, avoiding host buffer staging in the user code. For this lab it is sufficient to know that you can directly pass GPU pointers to the MPI routines of a CUDA-aware MPI implementation. If you want to learn more about CUDA-aware MPI I recommend you to read my post on the Parallel Forall blog: \n",
    "[An Introduction to CUDA-Aware MPI](http://devblogs.nvidia.com/parallelforall/introduction-cuda-aware-mpi/)\n",
    "\n",
    "### <code>!$acc host_data use_device( A )</code>\n",
    "In an OpenACC data region a CPU and a GPU copy of each scalar or array exists. Since MPI calls are executed by the CPU the default behavior of an OpenACC program is to use the CPU copy for these calls. In an OpenACC accelerated program however usually the GPU copy of the data is the one operated on so passing the CPU copy into MPI would result in communication of stale data and thus wrong results. The default behavior can be changed with the directive <code>host_data use_device</code>. <code>host_data use_device(A)</code> tells the OpenACC compiler to use the device representation of <code>A</code> in the following code block. Because we are using a CUDA-aware MPI the MPI implementation can handle these and do the halo updates directly to and from GPU memory.\n",
    "\n",
    "## Structure of this Lab\n",
    "\n",
    "This lab is broken up into five tasks; instructions for each will be provided in-line below. The \"solution\" to each task is the starting point of the next task, so you can skip around if you'd like. In addition, reference solutions are provided for each task. You can find them by looking for files matching \\*.solution\\*. You can build and run the solution with the make target <code>task?.solution</code>, e.g. for task 1: <code>make -C FORTRAN task1.solution</code>.\n",
    "\n",
    "Instructions for downloading this IPython Notebook, as well as a .zip file of the source you worked on, are provided at the bottom of the lab in the <a href=\"#post-lab\">Post Lab</a> section.\n",
    "\n",
    "\n",
    "## 2D Poisson Solver\n",
    "\n",
    "The code used in this lab is a Jacobi solver for the 2D Poisson equation on a rectangle:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "$\\LARGE{\\Delta A(x,y) = \\LARGE{e^{-10 * (x^2 + y^2)}} \\; \\forall (x,y) \\in \\Omega\\backslash\\delta\\Omega}$\n",
    "\n",
    "</div>\n",
    "\n",
    "With periodic boundary conditions.\n",
    "\n",
    "Given a 2D grid of vertexes, the solver attempts to set every vertex equal to the average of neighboring vertices.  It will iterate until the system converges to a stable value. So in each iteration of the Jacobi solver for all interior vertices \n",
    "\n",
    "<div align=\"center\"><img src=\"files/laplace2d.PNG\" width=\"30%\" />$\\LARGE{A_{k+1}(i,j)} = \\frac{rhs(i,j) - A_{k}(i-1,j) + A_{k}(i + 1,j) + A_{k}(i,j-1) + A_{k}(i,j+1)}{4}$</div>\n",
    "\n",
    "is applied and then the periodic boundary conditions are handled by copying\n",
    "\n",
    "1. the values of the first interior column to the right boundary and the last interior column to the left boundary:\n",
    "    <div align=\"center\">\n",
    "    <img src=\"files/jacobi_iteration_C.PNG\" width=\"30%\" />\n",
    "    </div>\n",
    "2. the values of the first interior row to the bottom boundary and the last interior row to the top boundary:\n",
    "    <div align=\"center\">\n",
    "    <img src=\"files/jacobi_iteration_B.PNG\" width=\"30%\" />\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the NVIDIA Visual Profiler (NVVP)\n",
    "\n",
    "As described in [CUDA Pro Tip: Profiling MPI Applications](http://devblogs.nvidia.com/parallelforall/cuda-pro-tip-profiling-mpi-applications/) <code>nvprof</code> can be used to generate profiles of MPI+OpenACC applications. Execute the <code>make -C FORTRAN task?.profile</code> in the one of the cells below to generate a profile for each MPI rank executing your solution of the selected task.\n",
    "\n",
    "To view the generated profiles we'll be using the NVIDIA Visual Profiler (NVVP) tool which comes standard with the CUDA Toolkit software.  To launch the tool please <a href=\"/vnc\" onclick=\"window.open(this.href, 'ProfilerVNC',\n",
    "'left=20,top=20,width=1290,height=730,toolbar=1,resizable=0'); return false;\">click here</a> which will open a new browser window.  **Note that it may take a few seconds for NVVP to start.**\n",
    "\n",
    "After NVVP has started, import the generated profiles by clicking on \"File\" and then \"Import...\". In the dialog select \"nvprof\" and \"Multiple Processes\". Browse to <code>ubuntu/notebook/FORTRAN/task?</code> and select <code>poisson2d.[0-3].nvvp</code>.\n",
    "\n",
    "If you've never used NVVP before or if you want to read more about you can [click here](https://developer.nvidia.com/nvidia-visual-profiler) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Tasks\n",
    "\n",
    "** This is a long lab, so please pay attention to your time.**  You have 120 minutes of access time from when the lab connection information was presented to you.  You may want to pre-download the IPython Notebook and source in the <a href=\"#post-lab\">Post Lab</a> before continuing.\n",
    "\n",
    "## Task #1\n",
    "\n",
    "If you execute the initial version with `make -C FORTRAN task1` in the cell below you will see that the domain decomposition used to distribute the work across multiple GPUs delivered a speed-up but does not attain optimal efficiency. This is because of the time that is needed (spent) to carry out the halo updates using MPI. This wasted time is called \"parallel overhead\" because it is a step not necessary for execution with a single GPU. We can lower the parallel overhead by doing computations in parallel with the MPI communication and therefore hide the communication time. In the case of our Jacobi solver, this is best done by splitting each domain into a boundary part (which updates all values that we need to communicate) and an inner part. By doing this split, we can start the MPI communication after the boundary part has finished, and let it run in parallel with the inner part:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src=\"files/overlapping_mpi_compute.png\" width=\"80%\" />\n",
    "\n",
    "</div>\n",
    "\n",
    "In OpenACC this can be done by using the async clause on a kernels region as outlined below.\n",
    "\n",
    "```fortran\n",
    "!$acc kernels\n",
    "DO\n",
    "     !Process boundary\n",
    "END DO\n",
    "!$acc end kernels\n",
    "\n",
    "!$acc kernels async\n",
    "DO\n",
    "     !Process boundary\n",
    "END DO\n",
    "!$acc end kernels\n",
    "\n",
    "!$acc host_data use_device( a )\n",
    "  !Exchange halo with top and bottom neighbor\n",
    "  CALL MPI_Sendrecv( a…)\n",
    "  !…\n",
    "!$acc end host_data\n",
    "!wait for iteration to finish\n",
    "!$acc wait\n",
    "```\n",
    "\n",
    "In this task you should apply this approach to the copy loop of the Jacobi solver. Look out for <code>TODO</code> in <code>FORTRAN/task1/poisson2d.F03</code>. These TODOs will guide you through the following steps:\n",
    "\n",
    "* Split the copy loop into its constituent halo and bulk parts.\n",
    "* Start the computation of the bulk part asynchronously.\n",
    "* Wait for the bulk part to complete at the end of the iteration.\n",
    "\n",
    "To compile and run simply issue <code>make -C FORTRAN task1</code> as given in the cell below.\n",
    "\n",
    "The following reference might be interesting for you:\n",
    "\n",
    "* API documentation for MPI from the OpenMPI website [https://www.open-mpi.org/doc/v1.8](https://www.open-mpi.org/doc/v1.8).\n",
    "* [OpenACC 2.0 Quick Reference Guide](http://104.239.134.127/sites/default/files/213462%2010_OpenACC_API_QRG_HiRes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"FORTRAN/task1\" src=\"FORTRAN/task1\" width=\"100%\" height=\"600px\">\n",
    "  <p>Your browser does not support iframes.</p>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!make -C FORTRAN task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the output you will see a output similar to this:\n",
    "```\n",
    " Num GPUs:             4\n",
    "4096x4096: 1 GPU:   5.4184 s 4 GPUs:   1.6332 s, speedup:     3.32 efficiency:    82.94\n",
    "```\n",
    "So compared to the initial results\n",
    "```\n",
    " Num GPUs:             4\n",
    "4096x4096: 1 GPU:   5.4390 s 4 GPUs:   1.7262 s, speedup:     3.15 efficiency:    78.77\n",
    "```\n",
    "The parallel efficiency increased by about 4%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Decomposition Stragegies\n",
    "\n",
    "There are three major options for breaking up our 2D grid of vertexes, or domain, to parallelize the work across multiple GPUs. The halo region shown in light green in the images is the data that needs to be shared among the GPUs working on the problem.\n",
    "\n",
    "<div style=\"display: inline;\"><br><img src=\"files/decomp_tiles.PNG\" align=\"left\" width=\"15%\" />\n",
    "<ul style=\"position: relative; left: 15px; vertical-align: middle;\">Minimizes surface area/volume ratio:\n",
    "<li style=\"position: relative; left: 30px;\">Communicate less data</li>\n",
    "<li style=\"position: relative; left: 30px;\">Optimal for bandwidth bound communication</li></ul></div>\n",
    "\n",
    "<div style=\"clear: left;\"><br><img src=\"files/decomp_vertical_stripes.PNG\" align=\"left\" width=\"15%\" />\n",
    "<ul style=\"position: relative; left: 15px;\">Minimizes number of neighbors:\n",
    "<li style=\"position: relative; left: 30px;\">Communicate to fewer neighbors</li>\n",
    "<li style=\"position: relative; left: 30px;\">Optimal for latency bound communication</li>\n",
    "Contiguous if data is [column-major](https://en.wikipedia.org/wiki/Column-major_order)</ul></div>\n",
    "\n",
    "<div style=\"clear: left;\"><br><img src=\"files/decomp_horizontal_stripes.PNG\" align=\"left\" width=\"15%\" />\n",
    "<br><ul style=\"position: relative; left: 15px;\">Minimizes number of neighbors:\n",
    "<li style=\"position: relative; left: 30px;\">Communicate to fewer neighbors</li>\n",
    "<li style=\"position: relative; left: 30px;\">Optimal for latency bound communication</li>\n",
    "Contiguous if data is [row-major](https://en.wikipedia.org/wiki/Row-major_order)</ul></div>\n",
    "\n",
    "<div style=\"clear: left;\"><br><br></div>\n",
    "\n",
    "The initial version of the Jacobi solver used in this lab applied a domain decomposition using vertical stripes as column-major order is used in FORTRAN. Since on the cloud nodes available for this lab only 4 GPUs are available this is also the optimal choice because:\n",
    "\n",
    "* it minimizes the number of neighbors we need to comunicate with\n",
    "* with only 4 GPUs the amount of data that needs to be communicated with all neighbors is the same for a domain decomposition with stripes and tiles\n",
    "\n",
    "However depending on the amount of data a given applicaiton needs to exchange and hardware properties like network bandwidth a tiled domain decompostion can be better when more GPUs are used. So even if it does not give a benefit for the available number of GPUs in this lab and for the given solver the remaining tasks modify the given solver to apply a 2D domain decomposition step by step.\n",
    "\n",
    "## Noncontiguous halo updates\n",
    "\n",
    "A domain decomposition with tiles requires to exchange noncontigous data between GPUs. Exchanging noncontiguous data between GPUs is inefficient because it requires to send many small messages. To avoid that a common strategy is to use staging buffers to send all data that needs to be exchanged with a neighbor in a single larger message. In case of our 2D Jacobi solver we need four staging buffers:\n",
    "\n",
    "* <code>to_top</code>: to gather data which needs to be send to top neighbor\n",
    "* <code>to_bottom</code>: to gather data which needs to be send to bottom neighbor\n",
    "* <code>from_top</code>: to receive data from top neighbor\n",
    "* <code>from_bottom</code>: to receive data from bottom neighbor\n",
    "\n",
    "Then before starting MPI communication with the top and bottom neighbor data is gathered into <code>to_top</code> and <code>to_bottom</code> and after the MPI communication the received data is scattered into the working buffer from <code>from_top</code> and <code>from_bottom</code>:\n",
    "\n",
    "```fortran\n",
    "!$acc kernels\n",
    "DO iy = iy_start, iy_end\n",
    "    to_top(iy) = a(ix_start,iy)\n",
    "    to_bottom(iy) = a(ix_end,iy)\n",
    "END DO\n",
    "!$acc end kernels\n",
    "!$acc host_data use_device( A )\n",
    "    !1. Sent to_top starting from first modified row (iy_start) to last modified row to top and receive the same rows into from_bottom from bottom\n",
    "    CALL MPI_Sendrecv( to_top(iy_start), (iy_end-iy_start)+1, MPI_REAL, top   , 0, from_bottom(iy_start), (iy_end-iy_start)+1, MPI_REAL, bottom, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE, ierror )\n",
    "\n",
    "    !2. Sent to_bottom starting from first modified row (iy_start) to last modified row to bottom and receive the same rows into from_top from top\n",
    "    CALL MPI_Sendrecv( to_bottom(iy_start), (iy_end-iy_start)+1, MPI_REAL, bottom   , 0, from_top(iy_start), (iy_end-iy_start)+1, MPI_REAL, top, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE, ierror )\n",
    "!$acc end host_data\n",
    "!$acc kernels\n",
    "DO iy = iy_start, iy_end\n",
    "    a(ix_start-1,iy) = from_top(iy)\n",
    "    a(ix_end+1,iy) = from_bottom(iy)\n",
    "END DO\n",
    "!$acc end kernels\n",
    "```\n",
    "\n",
    "Using MPI datatypes is an alternative to application managed staging buffers. Depending on the used MPI implementation that can even be more efficient because the MPI implementation can do the packing and unpacking in its internal pipeline and therefore better hide the packing and unpacking time.\n",
    "\n",
    "If we treat the top most rank responsible for a certain set of columns as the bottom neighbour of the rank at the bottom responsible for the same set of columns and the rank at the bottom as the top neighbour of the rank at the top doing the top/bottom halo update will also handle the periodic boundary conditions and therefore the loop handling those can be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #2\n",
    "\n",
    "The purpose of this and the following tasks is to change the domain decomposition from vertical stripes to a tiled domain decomposition. To let you focus on the logical domain decomposition and GPU to GPU communication the data here is still fully replicated on each GPU. (This is something one would normally not do, as it has a significant storage overhead, but it avoids some boilerplate code that should not be part of this lab.). The process of this transformation is brocken up into multiple smaller steps. In Task 2 logical 2D MPI ranks should be introduced.\n",
    "\n",
    "Like in Task #1 you should look out for <code>TODO</code> in <code>FORTRAN/task2/poisson2d.c</code>. These will guide you through the following steps:\n",
    "\n",
    "* Use the provided method <code>size_to_2Dsize</code> to derive 2D MPI ranks and size from the given 1D MPI rank and size.\n",
    "* Map the 2D rank back to the MPI rank to be used in <code>MPI_Sendrecv</code>\n",
    "\n",
    "The MPI standard allows to create communicators with cartesian topology information attached. These communicators can be used to accheive the same (<code>MPI_Cart_create</code>). For educational purposes this is done manually in this lab.\n",
    "\n",
    "[Click here to see hints](#Task-#2---Hints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"FORTRAN/task2\" src=\"FORTRAN/task2\" width=\"100%\" height=\"600px\">\n",
    "  <p>Your browser does not support iframes.</p>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!make -C FORTRAN task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the output you will see a output similar to this:\n",
    "```\n",
    "Num GPUs: 4 with a (2,2) layout.\n",
    "4096x4096: 1 GPU:   5.4422 s 4 GPUs:   3.0413 s, speedup:     1.79 efficiency:    44.74\n",
    "```\n",
    "Since we are replicating the work of the two GPUs responsible for the same set of rows its expected that we get about half the parallel efficiency compared to the origingal code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #3\n",
    "\n",
    "In Task 3 the staging buffers <code>to_top,from_top,to_bottom,from_bottom</code> should be used to handle the periodic boundary conditions.\n",
    "\n",
    "Like in the other tasks you should look out for <code>TODO</code> in <code>FORTRAN/task3/poisson2d.c</code>. These will guide you through the following steps:\n",
    "\n",
    "* Gather data to be copied from first inner row to bottom boundary in <code>to_bottom</code>\n",
    "* Gather data to be copied from last inner row to top boundary in <code>to_top</code>\n",
    "* Scatter data to be copied from first inner row to top boundary from <code>from_bottom</code>\n",
    "* Scatter data to be copied from last inner row to bottom boundary from <code>from_top</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"FORTRAN/task3\" src=\"FORTRAN/task3\" width=\"100%\" height=\"600px\">\n",
    "  <p>Your browser does not support iframes.</p>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!make -C FORTRAN task3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the output you will see a output similar to this:\n",
    "```\n",
    "Num GPUs: 4 with a (2,2) layout.\n",
    "4096x4096: 1 GPU:   5.4306 s 4 GPUs:   3.1740 s, speedup:     1.71 efficiency:    42.77\n",
    "```\n",
    "Since we are still replicating the work of the two GPUs responsible for the same set of rows its expected that we do not see an improvment in parallel efficiency compared to the last task. This will be fixed in the next task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #4\n",
    "\n",
    "In Task 4 the actual domain decomposition of the columns should be applied.\n",
    "\n",
    "Like in the other tasks you should look out for <code>TODO</code> in <code>FORTRAN/task4/poisson2d.c</code>. These will guide you through the following steps:\n",
    "\n",
    "* set first (<code>ix_start</code>) and last (<code>ix_end<code>) column to be processed by each rank\n",
    "* replace the copy <code>from_top</code>,<code>to_bottom</code>,<code>from_bottom</code>,<code>to_top</code> loop with MPI communication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"FORTRAN/task4\" src=\"FORTRAN/task4\" width=\"100%\" height=\"600px\">\n",
    "  <p>Your browser does not support iframes.</p>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!make -C FORTRAN task4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the output you will see a output similar to this:\n",
    "```\n",
    "Num GPUs: 4 with a (2,2) layout.\n",
    "4096x4096: 1 GPU:   5.4268 s 4 GPUs:   1.7896 s, speedup:     3.03 efficiency:    75.81\n",
    "```\n",
    "As said above its expected that the parallel efficiency does not increase compared to the initial version using a domain decomposition with stripes. With only 4 GPUs the amount of data each GPU needs to exchange with its neighbors is the same for a tiled domain decomposition and a domain decomposition with stripes. Furthermore the tiled domain decomposition has a higher parallel overhead because of the necessary gather and scatter steps. With more GPUs this can change and doing the gather and scatter steps will pay off and lead to better scalabilty of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Task #5\n",
    "\n",
    "This is an optional task to apply communication computation overlap to the tiled domain decomposition version as it was done above in task 1 for the inital version using a domain decompostion with stripes. The aim is hide as much of the parallel overhead (MPI, gather, scatter) as possible. As in the other tasks follow the TODOs in <code>FORTRAN/task5/poisson2d.c</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"FORTRAN/task5\" src=\"FORTRAN/task5\" width=\"100%\" height=\"600px\">\n",
    "  <p>Your browser does not support iframes.</p>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!make -C FORTRAN task5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you have learned how to improve the scalability of a MPI+OpenACC multi GPU application by hiding communicaiton times and applying a 2D tiled domain decomposition to decrease the amount of data that needs to be exchanged between GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"post-lab\"></a>\n",
    "## Post-Lab\n",
    "\n",
    "Finally, don't forget to save your work from this lab before time runs out and the instance shuts down!!\n",
    "\n",
    "1. Save this IPython Notebook by going to `File -> Download as -> IPython (.ipynb)` at the top of this window\n",
    "2. You can execute the following cell block to create a zip-file of the files you've been working on, and download it with the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -f advanced_multi_gpu_mpi_openacc_files.zip\n",
    "zip -r advanced_multi_gpu_mpi_openacc_files.zip C FORTRAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After** executing the above cell, you should be able to download the zip file [here](files/advanced_multi_gpu_mpi_openacc_files.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"References/Further Reading\"></a>\n",
    "## References/Further Reading\n",
    "\n",
    "* Learn more at the [CUDA Developer Zone](https://developer.nvidia.com/category/zone/cuda-zone).\n",
    "* If you have an NVIDIA GPU in your system, you can download and install the [CUDA tookit](https://developer.nvidia.com/cuda-toolkit).\n",
    "* Take the fantastic online and **free** Udacity [Intro to Parallel Programming](https://www.udacity.com/course/cs344) course which uses CUDA C.\n",
    "* Search or ask questions on [Stackoverflow](http://stackoverflow.com/questions/tagged/cuda) using the cuda tag\n",
    "* Read the GPU Computing developer blog [Parallel Forall](http://devblogs.nvidia.com/parallelforall/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"FAQ\"></a>\n",
    "---\n",
    "# Lab FAQ\n",
    "\n",
    "Q: I'm encountering issues executing the cells, or other technical problems?<br>\n",
    "A: Please see [this](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) infrastructure FAQ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "### Task #2 - Hints\n",
    "#### Hint #1\n",
    "Modulo (<code>MOD</code>) and devide (<code>/</code>) are your friend.\n",
    "\n",
    "#### Hint #2\n",
    "You can use the following to compute 2D MPI rank and size from the provided 1D MPI rank and size.\n",
    "```\n",
    "CALL size_to_2Dsize(mpi_size, mpi_sizex, mpi_sizey)    \n",
    "mpi_rankx = MOD( mpi_rank, mpi_sizex )\n",
    "mpi_ranky = mpi_rank/mpi_sizex\n",
    "```\n",
    "\n",
    "[Return to Task #2](#Task-#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "p.hint_trigger{\n",
    "  margin-bottom:7px;\n",
    "  margin-top:-5px;\n",
    "  background:#64E84D;\n",
    "}\n",
    ".toggle_container{\n",
    "  margin-bottom:0px;\n",
    "}\n",
    ".toggle_container p{\n",
    "  margin:2px;\n",
    "}\n",
    ".toggle_container{\n",
    "  background:#f0f0f0;\n",
    "  clear: both;\n",
    "  font-size:100%;\n",
    "}\n",
    "</style>\n",
    "<script>\n",
    "$(\"p.hint_trigger\").click(function(){\n",
    "   $(this).toggleClass(\"active\").next().slideToggle(\"normal\");\n",
    "});\n",
    "   \n",
    "$(\".toggle_container\").hide();\n",
    "</script>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
